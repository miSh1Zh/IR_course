\section{Реализация}

\subsection{Структура проекта}

\begin{verbatim}
ir/
├── crawler/
│   └── medical_crawler/
│       └── spiders/
│           ├── journaldoctor.py
│           ├── rmj.py
│           ├── wikipedia.py
│           ├── probolezny.py
│           ├── ruwiki.py
│           ├── bigenc.py
│           ├── bnews.py
│           ├── takzdorovo.py
│           └── clinickrasnodar.py
├── engine/
│   ├── src/
│   │   ├── tokenizer.cpp/hpp
│   │   ├── stemmer.cpp/hpp
│   │   ├── hashmap.hpp
│   │   ├── indexer.cpp/hpp
│   │   ├── searcher.cpp/hpp
│   │   ├── query_parser.cpp/hpp
│   │   ├── main_indexer.cpp
│   │   └── main_searcher.cpp
│   └── tests/
├── web/
└── analysis/
\end{verbatim}

\subsection{Токенизатор}

Класс \texttt{Tokenizer} реализует разбиение текста на токены с поддержкой UTF-8:

\begin{lstlisting}[style=customc]
class Tokenizer {
public:
    std::vector<std::string> tokenize(const std::string& text);
private:
    bool is_alpha_utf8(const std::string& str, size_t& pos, 
                       std::string& char_out);
    std::string to_lowercase_utf8(const std::string& ch);
};
\end{lstlisting}

\subsection{Хеш-таблица}

Реализация на основе метода цепочек:

\begin{lstlisting}[style=customc]
template<typename V>
class HashMap {
    struct Node {
        std::string key;
        V value;
        Node* next;
    };
    std::vector<Node*> buckets;
    
    size_t hash(const std::string& key) const {
        size_t h = 5381;
        for (char c : key)
            h = ((h << 5) + h) + c;
        return h % buckets.size();
    }
};
\end{lstlisting}

\subsection{Краулеры (Spider'ы)}

Реализовано 9 spider'ов для различных источников данных:

\begin{itemize}
    \item \textbf{journaldoctor.py} --- рекурсивный обход журнала journaldoctor.ru
    \item \textbf{rmj.py} --- динамическое обнаружение категорий на rmj.ru, обход с пагинацией
    \item \textbf{wikipedia.py} --- обход медицинских категорий Википедии
    \item \textbf{probolezny.py} --- навигация по категориям заболеваний на probolezny.ru
    \item \textbf{ruwiki.py} --- рекурсивный обход медицинской категории РУВИКИ
    \item \textbf{bigenc.py} --- обход категорий Большой Российской Энциклопедии
    \item \textbf{bnews.py} --- парсинг разделов журнала b-news.media
    \item \textbf{takzdorovo.py} --- сбор статей из раздела /stati/ портала takzdorovo.ru
    \item \textbf{clinickrasnodar.py} --- парсинг статей с обработкой Punycode URL (клиникакраснодар.рф)
\end{itemize}

Все spider'ы используют:
\begin{itemize}
    \item ROBOTSTXT\_OBEY: False (для эффективной сборки)
    \item JOBDIR для сохранения состояния (предотвращение повторного обхода)
    \item Уникальный индекс в MongoDB на поле URL (дедупликация)
\end{itemize}

\subsection{Парсер запросов

Рекурсивный спуск для разбора булевых выражений:

\begin{verbatim}
Грамматика:
  query   -> or_expr
  or_expr -> and_expr (|| and_expr)*
  and_expr-> not_expr ((&& | space) not_expr)*
  not_expr-> !? primary
  primary -> term | (query)
\end{verbatim}

\subsection{Бинарный формат индекса}

\begin{verbatim}
Header (32 bytes):
  - magic: 4 bytes ("MIDX")
  - version: 4 bytes
  - num_terms: 4 bytes
  - num_docs: 4 bytes
  - forward_offset: 8 bytes
  - reserved: 8 bytes

Inverted index:
  For each term:
    - term_len: 4 bytes
    - term: term_len bytes (UTF-8)
    - posting_len: 4 bytes
    - postings: posting_len * 4 bytes (uint32[])

Forward index (at forward_offset):
  For each document:
    - doc_id: 4 bytes
    - title_len, title: ...
    - url_len, url: ...
    - category_len, category: ...
    - source_len, source: ...
\end{verbatim}

\pagebreak

